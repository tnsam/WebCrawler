{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ong T'nsam_Web Crawler(New Straits Times)(With next link).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKHERoSTfoBD",
        "outputId": "1081fd1d-9871-477a-d5c5-1642deaef011"
      },
      "source": [
        "# install chromium, its driver, and selenium\n",
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "# set options to be headless, ..\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Ign:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [800 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,414 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [480 kB]\n",
            "Hit:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,770 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [450 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,184 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,184 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [33.5 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,616 kB]\n",
            "Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [906 kB]\n",
            "Get:26 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.9 kB]\n",
            "Fetched 13.2 MB in 7s (1,827 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 48 not upgraded.\n",
            "Need to get 86.0 MB of archives.\n",
            "After this operation, 298 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 91.0.4472.77-0ubuntu0.18.04.1 [1,124 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 91.0.4472.77-0ubuntu0.18.04.1 [76.1 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 91.0.4472.77-0ubuntu0.18.04.1 [3,948 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 91.0.4472.77-0ubuntu0.18.04.1 [4,838 kB]\n",
            "Fetched 86.0 MB in 10s (9,037 kB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 160772 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_91.0.4472.77-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (91.0.4472.77-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_91.0.4472.77-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (91.0.4472.77-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_91.0.4472.77-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (91.0.4472.77-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_91.0.4472.77-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (91.0.4472.77-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (91.0.4472.77-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (91.0.4472.77-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (91.0.4472.77-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (91.0.4472.77-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTfBfexffrHL",
        "outputId": "b3e4fc1c-82da-439f-f946-2d4d037edd40"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJG3XMs7fo3C"
      },
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import pathlib\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from pytz import timezone\n",
        "\n",
        "def crawl_content(url, category) :\n",
        "  driver = webdriver.Chrome('chromedriver',options=options)  \n",
        "\n",
        "  driver.get(url)\n",
        "\n",
        "  # Wait for initialize, in seconds\n",
        "  wait = WebDriverWait(driver, 20)\n",
        "\n",
        "  #obtain the link for all the category tabs\n",
        "  category_link = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a.nav-item.d-inline-block.py-3.px-3\")))\n",
        "  category_link_list = []\n",
        "    \n",
        "  for i in category_link:\n",
        "    links = i.get_property('href')\n",
        "    category_link_list.append(links)\n",
        "  \n",
        "  #crawl the content for all the news under each category tab\n",
        "  c = 0\n",
        "  for c in category_link_list:  \n",
        "    if(c[-5:] != \"video\"):\n",
        "\n",
        "      reload = True\n",
        "      while reload:  \n",
        " \n",
        "        load = True\n",
        "        while load:\n",
        "          try:\n",
        "            driver.get(c)\n",
        "            time.sleep(8)\n",
        "            field_category = driver.find_element_by_css_selector('span.field-category').text\n",
        "            load = False \n",
        "\n",
        "          except:\n",
        "            load = True\n",
        "\n",
        "        category_tab = driver.execute_script(\"return document.querySelector('div.category-nav > a.nav-item.d-inline-block.py-3.px-3.active').innerText\")\n",
        "\n",
        "        if category_tab == 'BUSINESS TIMES':\n",
        "          category_tab = 'BUSINESS'\n",
        "\n",
        "        if field_category != category_tab:\n",
        "          reload = True\n",
        "\n",
        "        else:\n",
        "          reload = False \n",
        "\n",
        "      #check the last news article crawled\n",
        "      csv_file_checking = 'drive/My Drive/fyp/seleniumProject/checking_news.csv'\n",
        "      file = pathlib.Path(csv_file)\n",
        "      file2 = pathlib.Path(csv_file_checking)\n",
        "      article_list = []\n",
        "      count = 0\n",
        "      nextPage = True\n",
        "      while nextPage:    \n",
        "        time.sleep(8)   \n",
        "        all_news_link = driver.find_elements_by_css_selector(\"a.d-flex.article.listing.mb-3.pb-3\")\n",
        "\n",
        "        listOflinks=[]\n",
        "        for el in all_news_link:\n",
        "          link = el.get_property('href')\n",
        "          listOflinks.append(link)\n",
        "\n",
        "        time.sleep(8)        \n",
        "                \n",
        "        valid = True\n",
        "        condition = False\n",
        "        i = 0\n",
        "        while (i < len(listOflinks)) and valid:\n",
        "          \n",
        "          driver.get(listOflinks[i])\n",
        "          time.sleep(8)\n",
        "\n",
        "          #time created of the news articles\n",
        "          time_created = driver.execute_script(\"return document.querySelector('div.article-meta > div:last-child').innerText.split('-').pop().trim();\")\n",
        "          time_created = datetime.strptime(time_created, '%B %d, %Y  @ %I:%M%p')\n",
        "          timezone = pytz.timezone(\"Asia/Kuala_Lumpur\")\n",
        "          time_created = time_created.astimezone(timezone)\n",
        "          time_created = time_created.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "          #title\n",
        "          title = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#main > div > div > div.col > div:nth-child(1) > div > h1 > span\"))).text\n",
        "          \n",
        "          #author\n",
        "          try : \n",
        "            author =  wait.until(EC.presence_of_element_located((By.XPATH,\"//*[@id='main']/div/div/div[1]/div[1]/div/div/div[1]/div[1]/div/span/a\"))).text\n",
        "          \n",
        "          except:\n",
        "            author = \" \"\n",
        "        \n",
        "          #content\n",
        "          content = wait.until(EC.presence_of_all_elements_located((By.TAG_NAME,\"p\")))\n",
        "\n",
        "          all_content = []\n",
        "\n",
        "          for j in content:\n",
        "            all_content.append(j.text)\n",
        "          \n",
        "          if file.exists() and file2.exists():\n",
        "              existed_news = pd.read_csv(csv_file_checking)\n",
        "\n",
        "              record = existed_news[existed_news[\"Subcategory\"] == field_category]\n",
        "              if len(record.index) != 0 :\n",
        "                existed_date = record['Time Created'].iloc[0]\n",
        "                existed_title = record['Title'].iloc[0]\n",
        "\n",
        "                date_time = datetime.strptime(time_created, \"%Y-%m-%d %H:%M:%S\")\n",
        "                exist_date = datetime.strptime(existed_date, \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "                if (date_time >= exist_date) and (title != existed_title):\n",
        "                  valid = True\n",
        "                  article_details = {'Category': category, 'Subcategory': field_category, 'Time Created': time_created, 'Title' : title, 'Author' : author, 'Content' : all_content}\n",
        "                  article_list.append(article_details)\n",
        "\n",
        "                  if i == (len(listOflinks)-1):\n",
        "                    condition = True\n",
        "\n",
        "                  else:\n",
        "                    condition = False                  \n",
        "\n",
        "                else:\n",
        "                  valid = False\n",
        "                  condition = False \n",
        "                \n",
        "              else:\n",
        "                article_details = {'Category': category, 'Subcategory': field_category, 'Time Created': time_created, 'Title' : title, 'Author' : author, 'Content' : all_content}\n",
        "                article_list.append(article_details)    \n",
        "                condition = True                          \n",
        "\n",
        "          else:\n",
        "            article_details = {'Category': category, 'Subcategory': field_category, 'Time Created': time_created, 'Title' : title, 'Author' : author, 'Content' : all_content}\n",
        "            article_list.append(article_details)\n",
        "            condition = True\n",
        "\n",
        "          i += 1\n",
        "\n",
        "        #check if need to go to the next page\n",
        "        if condition:\n",
        "          driver.get(c)\n",
        "          count += 1\n",
        "          a = 0\n",
        "          while a < count: \n",
        "            try:\n",
        "              #go to next page\n",
        "              driver.find_element_by_link_text(\"Next »\").click()\n",
        "              nextPage = True\n",
        "            \n",
        "            except:\n",
        "              nextPage = False\n",
        "            a += 1\n",
        "\n",
        "        else:\n",
        "          nextPage = False\n",
        "\n",
        "      if article_list != []:\n",
        "        results = pd.DataFrame(article_list)\n",
        "        results[\"Time Created\"] = pd.to_datetime(results[\"Time Created\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "        results = results.sort_values(by=\"Time Created\")\n",
        "            \n",
        "        if file.exists ():\n",
        "            results.to_csv(csv_file, mode = 'a', index=False, header = False)\n",
        "        else:\n",
        "            results.to_csv(csv_file, index=False)\n",
        "        \n",
        "        last_row = pd.DataFrame(results.tail(1))\n",
        "        if file2.exists ():\n",
        "          record = existed_news[existed_news[\"Subcategory\"] == field_category]\n",
        "          if len(record) != 0:\n",
        "            last_record_datetime = last_row['Time Created'].iloc[0]\n",
        "\n",
        "            existed_news.loc[existed_news[\"Subcategory\"] == field_category, 'Time Created'] = last_record_datetime\n",
        "            existed_news.loc[existed_news[\"Subcategory\"] == field_category, 'Title'] = last_row['Title'].iloc[0]\n",
        "            existed_news.to_csv(csv_file_checking, index=False)\n",
        "\n",
        "          else:\n",
        "            last_row.to_csv(csv_file_checking, mode = 'a', index=False, header = False)\n",
        "\n",
        "        else:\n",
        "          last_row.to_csv(csv_file_checking, index=False)\n",
        "  \n",
        "\n",
        "csv_file = 'drive/My Drive/fyp/seleniumProject/news.csv' \n",
        "crawl_content('https://www.nst.com.my/business', 'BUSINESS')\n",
        "crawl_content('https://www.nst.com.my/lifestyle', 'LIFESTYLE')\n",
        "crawl_content('https://www.nst.com.my/sports', 'SPORTS')\n",
        "crawl_content('https://www.nst.com.my/world', 'WORLD')\n",
        "crawl_content('https://www.nst.com.my/opinion', 'OPINION')\n",
        "\n",
        "final_results = pd.read_csv('drive/My Drive/fyp/seleniumProject/news.csv')\n",
        "final_results[\"Time Created\"] = pd.to_datetime(final_results[\"Time Created\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "final_results = final_results.sort_values(by=\"Time Created\")\n",
        "final_results = final_results.drop_duplicates()\n",
        "final_results.to_csv(csv_file, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}